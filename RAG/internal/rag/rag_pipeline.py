from haystack import Pipeline
from haystack.utils import Secret
from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore
from haystack_integrations.components.retrievers.elasticsearch import ElasticsearchEmbeddingRetriever
from haystack.components.generators import OpenAIGenerator
from haystack.components.builders.prompt_builder import PromptBuilder
from haystack.components.embedders import SentenceTransformersTextEmbedder, SentenceTransformersDocumentEmbedder
from haystack.document_stores.types import DuplicatePolicy
from internal.config.config import Config

class RAGPipeline:
    def __init__(self, config: Config = None):
        if config is None:
            config = Config()
            
        es_config = config.get_elasticsearch_config()
        llm_config = config.get_llm_config()
        
        # å®šä¹‰ç´¢å¼•æ˜ å°„
        custom_mapping = {
           "properties": {
                "content": {"type": "text"},
                "embedding": {
                    "type": "dense_vector",
                    "dims": 1024,  
                    "similarity": "cosine"
                },
                "meta": {
                    "properties": {
                        #"question":{"type": "text"},
                        "answer":{"type":"text"},
                        "id": {"type": "long"},
                        #"author_id": {"type": "keyword"},
                        #"author_name":{"type":"text"},
                        #"created_at": {"type": "date"},
                        #"tag": {"type": "text"},
                        "post_id":{"type":"text"}
                    }
                }
            }
        }
        
        # Initialize document store with embedding configuration
        self.document_store = ElasticsearchDocumentStore(
            hosts=es_config["hosts"][0],
            index="qqq",
            embedding_similarity_function="cosine",  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            custom_mapping=custom_mapping
        )
        self.llm_config = llm_config
        self.model_name = llm_config["embedding_model"]
        self.pipeline = self._create_pipeline()
        
    def _create_pipeline(self):
        prompt_template = """
        ä½ å«å°çŸ¥ï¼Œæ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾åŒºé—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œåˆ†æç”¨æˆ·æ‰€æé—®é¢˜å’Œæ–‡æ¡£ä¸­é—®é¢˜çš„ç›¸ä¼¼åº¦ï¼Œè¿”å›ç›¸ä¼¼åº¦å¹¶ç”¨ä¸“ä¸šä¸”å‹å¥½çš„è¯­æ°”å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·è¯šå®çš„å‘Šè¯‰ç”¨æˆ·æœªæ‰¾åˆ°ç›¸å…³é—®ç­”ï¼Œå¹¶æç¤ºå¯ä»¥å»é—®ç­”ç¤¾åŒºè¿›è¡Œå‘å¸–è¯¢é—®ã€‚
        å¦‚æœæ–‡æ¡£ä¸­æœ‰ç±»ä¼¼æ–‡æ¡£å†…å®¹ï¼Œè¯·åœ¨å…·ä½“å›ç­”ä¸­è¿”å›è¯¥é—®é¢˜çš„post_idï¼Œä»¥ä¾¿åšå‡ºä¸‹ä¸€æ­¥è·³è½¬æ“ä½œ,å¹¶è¯·æ ¹æ®æ–‡æ¡£ä¸­ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        æ‰€æœ‰ç­”æ¡ˆä¸ä¸ºç©ºçš„é—®ç­”æ–‡æ¡£éƒ½ä¸è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œåªç”Ÿæˆå›ç­”ã€‚
        é¢å¤–ä¿¡æ¯ï¼šåä¸­å¸ˆèŒƒå¤§å­¦åˆ«åå¯ä»¥æ˜¯ccnu,ä¹Ÿå¯ä»¥æ˜¯å˜»å˜»æ©å°¤ç­‰ç­‰ã€‚

        æ–‡æ¡£å†…å®¹:
        {% for doc in documents %}
              - é—®é¢˜å’Œç­”æ¡ˆ: {{ doc.content }}
              - å¸–å­ID:{{ doc.meta.post_id }}
              - å¸–å­ç­”æ¡ˆï¼š{{doc.meta.answer}}
        {% endfor %}


        é—®é¢˜: {{question}}

        å›ç­”ç»“æ„:
        - **ç›¸ä¼¼åº¦**: è¾“å‡ºé—®é¢˜å’Œæ–‡æ¡£ä¸­å†…å®¹çš„ç›¸ä¼¼åº¦ï¼Œå–å€¼èŒƒå›´ä¸º 0 åˆ° 1ã€‚
        - **å…·ä½“å›ç­”**: åŸºäºæ–‡æ¡£å†…å®¹ï¼Œæä¾›è¯¦ç»†çš„ç­”æ¡ˆã€‚
        - **å¸–å­ID**:åŸºäºæ–‡æ¡£å†…å®¹æŸ¥æ‰¾ç›¸ä¼¼çš„å¸–å­ï¼Œå¦‚æœæ–‡æ¡£ä¸­å­˜åœ¨è¯¥é—®é¢˜è€ŒæœªæŸ¥è¯¢åˆ°ç­”æ¡ˆï¼Œè¯·æŸ¥æ‰¾å¹¶è¿”å›è¯¥é—®é¢˜çš„post_idã€‚


        è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {
          "similarity": <ç›¸ä¼¼åº¦å€¼>,
          "answer": "<å…·ä½“å›ç­”>",
          "postids":"<å¸–å­ID>"
        }
        """
        
        # Initialize embedders
        self.text_embedder = SentenceTransformersTextEmbedder(model=self.model_name)
        self.text_embedder.warm_up()

        # Initialize retriever with embedding configuration
        self.retriever = ElasticsearchEmbeddingRetriever(
            document_store=self.document_store,
            top_k=5
        )
        
        self.prompt_builder = PromptBuilder(template=prompt_template)
        
        self.llm = OpenAIGenerator(
            api_key=Secret.from_token(self.llm_config["api_key"]),
            model=self.llm_config["model"],
            api_base_url=self.llm_config["base_url"],
        )
        
        pipeline = Pipeline()
        pipeline.add_component("text_embedder", self.text_embedder)
        pipeline.add_component("retriever", self.retriever)
        pipeline.add_component("prompt_builder", self.prompt_builder)
        pipeline.add_component("llm", self.llm)

        # Connect components
        pipeline.connect("text_embedder.embedding", "retriever.query_embedding")
        pipeline.connect("retriever", "prompt_builder.documents")
        pipeline.connect("prompt_builder", "llm")
        
        return pipeline
        
    def query(self, question: str):
        print(f"â“ å¤„ç†æŸ¥è¯¢: {question}")
        print("ğŸ”„ è¿è¡Œ RAG pipeline...")
        results = self.pipeline.run(
            {
                "text_embedder": {"text": question},
                "prompt_builder": {"question": question},
            }
        )
        print("âœ… RAG pipeline è¿è¡Œå®Œæˆ")
        return results
        
    def query_related_documents(self, question: str):
        print(f"â“ å¤„ç†æŸ¥è¯¢: {question}")
        print("ğŸ”„ è¿è¡Œ RAG pipeline...")
        embedding = self.text_embedder.run(question)

        results = self.retriever.run(embedding['embedding'])
        return results
    
