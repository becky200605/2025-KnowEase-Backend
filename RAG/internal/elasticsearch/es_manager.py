from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore
from haystack import Document
from haystack.document_stores.types import DuplicatePolicy
from haystack.components.embedders import SentenceTransformersDocumentEmbedder
from typing import List, Dict, Set
from internal.config.config import Config
from elasticsearch import Elasticsearch, NotFoundError

class ElasticsearchManager:
    def __init__(self, config: Config = None):
        if config is None:
            config = Config()
        es_config = config.get_elasticsearch_config()
        

        self.client = Elasticsearch(
            hosts=es_config["hosts"][0],
            verify_certs=False
        )
        
        # å®šä¹‰ç´¢å¼•æ˜ å°„
        self.custom_mapping = {
            "properties": {
                "content": {"type": "text"},
                "embedding": {
                    "type": "dense_vector",
                    "dims": 1024,  
                    "similarity": "cosine"
                },
                "meta": {
                    "properties": {
                        #"question":{"type": "text"},
                        "answer":{"type":"text"},
                        "id": {"type": "long"},
                        "post_id":{"type":"text"},
                        #"author_id": {"type": "keyword"},
                        #"author_name":{"type":"text"},
                        #"created_at": {"type": "date"},
                        #"tag": {"type": "text"}
                    }
                }
            }
        }
        
        self.index_name = "qqq"
        self._ensure_index_exists()
        
        self.document_store = ElasticsearchDocumentStore(
            hosts=es_config["hosts"][0],
            index=self.index_name,
            embedding_similarity_function="cosine",  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            custom_mapping=self.custom_mapping
        )
        
        self.model_name = "BAAI/bge-large-zh-v1.5"
        self.document_embedder = SentenceTransformersDocumentEmbedder(model=self.model_name)
        self.document_embedder.warm_up()
        
        # ç”¨äºç¼“å­˜å·²å­˜åœ¨çš„æ–‡æ¡£ID
        self._existing_ids: Set[int] = set()
        
        
    def _ensure_index_exists(self):
        """ç¡®ä¿ç´¢å¼•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        try:
            if not self.client.indices.exists(index=self.index_name):
                print(f"ğŸ“¦ åˆ›å»ºç´¢å¼• {self.index_name}...")
                self.client.indices.create(
                    index=self.index_name,
                    mappings=self.custom_mapping
                )
                print("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥/åˆ›å»ºç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
            raise
            
    def _check_document_exists(self, doc_id: int) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨"""
        try:
            # é¦–å…ˆç¡®ä¿ç´¢å¼•å­˜åœ¨
            if not self.client.indices.exists(index=self.index_name):
                return False
                
            query = {
                "query": {
                    "term": {
                        "meta.id": doc_id
                    }
                }
            }
            result = self.client.search(
                index=self.index_name,
                body=query,
                size=1
            )
            return result["hits"]["total"]["value"] > 0
        except NotFoundError:
            return False
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥æ–‡æ¡£å­˜åœ¨æ€§æ—¶å‡ºé”™: {str(e)}")
            return False
            
    def _filter_new_posts(self, posts: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤å‡ºæ–°çš„å¸–å­"""
        new_posts = []
        for post in posts:
            if not self._check_document_exists(post["id"]):
                new_posts.append(post)
            else:
                print(f"â© è·³è¿‡å·²å­˜åœ¨çš„å¸–å­: ID={post['id']}")
                
        skipped = len(posts) - len(new_posts)
        if skipped > 0:
            print(f"â© å…±è·³è¿‡ {skipped} æ¡å·²å­˜åœ¨çš„å¸–å­")
        return new_posts
        
    def index_posts(self, posts: List[Dict]):
        """ç´¢å¼•æ–°å¸–å­"""
        try:
            # ç¡®ä¿ç´¢å¼•å­˜åœ¨
            self._ensure_index_exists()
            
            # è¿‡æ»¤å·²å­˜åœ¨çš„å¸–å­
            new_posts = self._filter_new_posts(posts)
            if not new_posts:
                print("â„¹ï¸ æ²¡æœ‰æ–°çš„å¸–å­éœ€è¦ç´¢å¼•")
                return
                
            print(f"ğŸ“¥ ç´¢å¼• {len(new_posts)} æ¡æ–°æ–‡æ¡£...")
            documents = []
            for post in new_posts:
                doc = Document(
                    content=f"Question: {post['question']}\n\nAnswer: {post['answer']}",
                    meta={
                        #"question":post["question"],
                        "answer":post["answer"],
                        "id": post["id"],
                        "post_id":post["post_id"],
                        #"author_id": post["author_id"],
                        #"author_name": post["author_name"],
                        #"created_at": post["created_at"].isoformat(),
                        #"tag": post["tag"]
                    }
                )
                documents.append(doc)
            #print(documents)
                
            print("ğŸ“¥ ç”Ÿæˆæ–‡æ¡£åµŒå…¥...")
            documents_with_embeddings = self.document_embedder.run(documents)
            # æ‰“å°ç”Ÿæˆçš„å‘é‡å€¼
            #for doc in documents_with_embeddings["documents"]:
            #    print(f"æ–‡æ¡£ ID: {doc.meta['id']}, åµŒå…¥å‘é‡: {doc.embedding}")
                
            print("ğŸ’¾ å†™å…¥æ–‡æ¡£åˆ° Elasticsearch...")
            self.document_store.write_documents(
                documents_with_embeddings["documents"],
                policy=DuplicatePolicy.SKIP
            )
            #print(documents_with_embeddings)
            print("âœ… æ–‡æ¡£ç´¢å¼•å®Œæˆ")
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise
